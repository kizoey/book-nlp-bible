{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ch 19. 합성곱 신경망(CNN).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMpbAOSQ5ok8r7AlcSGJ+0l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_MtEujiIxKf3"},"source":["# 실습 [19-1]<br>\n","**실습명: CNN을 이용한 감성분석**<br>\n",": Kaggle의 아마존 리뷰 데이터를 활용해 binary 감성분석\n","- 90% 훈련 데이터, 10% 시험 데이터\n"]},{"cell_type":"code","metadata":{"id":"B7Hb08Oo6TzW"},"source":["#훈련 데이터 다운로드\n","from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVZTlQU54wJE"},"source":["#테스트 데이터 다운로드\n","from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEachGa_xB0u","executionInfo":{"status":"ok","timestamp":1624352347628,"user_tz":-540,"elapsed":3714,"user":{"displayName":"DA YEON KI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWfZD29yea7akQsMZ0L-JyyDBDzCak-4Q55tt-jw=s64","userId":"15097711384147757777"}}},"source":["#관련 라이브러리 불러오기\n","import os\n","import numpy as np\n","import nltk\n","import random\n","from keras.preprocessing.text import Tokenizer #토큰화 하는 tokenizer\n","from keras.preprocessing.sequence import pad_sequences #패딩 연산(0으로 채워넣는)에 필요한 메서드"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnrwinPE2UNA"},"source":["#데이터 읽기, 학습 데이터과 테스트 데이터로 분리\n","def read_dataset(dataset_type):\n","  max_seq_len = 0\n","  with open(\"/%s.txt\" % dataset_type, \"r\", encoding=\"utf-8\") as fr_handle:\n","    labels, sentences, tokenized_sentences = [], [], []\n","    for line in fr_handle:\n","      if line.strip() == 0:\n","        continue\n","      label = line.split(' ')[0]\n","      label = 0 if label == \"__label__1\" else 1 # 부정이면 0, 긍정이면 1\n","\n","      sentence = ' '.join(line.split(' ')[1:])\n","      tokenized_sentence = nltk.word_tokenize(sentence)\n","      max_seq_len = max(max_seq_len, len(tokenized_sentence))\n","\n","      labels.append(label)\n","      sentences.append(sentence)\n","    \n","    return labels, sentences, max_seq_len\n","\n","TRAIN_LABELS, TRAIN_SENTENCES, TRAIN_MAX_SEQ_LEN = read_dataset(\"train\")\n","TEST_LABELS, TEST_SENTENCES, TEST_MAX_SEQ_LEN = read_dataset(\"test\")\n","MAX_SEQUENCE_LEN = max(TRAIN_MAX_SEQ_LEN, TEST_MAX_SEQ_LEN)\n","\n","print(\"Train : \", len(TRAIN_SENTENCES))\n","for train_label, train_sent in zip(TRAIN_LABELS, TRAIN_SENTENCES[0:10]):\n","  print(train_label, ':' ,train_sent)\n","\n","print()\n","print(\"Test : \", len(TEST_SENTENCES))\n","for test_label, test_sent in zip(TEST_LABELS, TEST_SENTENCES[0:10]):\n","  print(test_label, ':' ,test_sent)\n","\n","print(\"MAX_SEQUENCE_LEN\", MAX_SEQUENCE_LEN)\n","with open(\"/content/vocab.txt\", \"r\", encoding=\"utf-8\") as vocab_handle:\n","  VOCAB = [line.strip() for line in vocab_handle if len(line.strip()) > 0]\n","  \n","  print(\"Total vocabulary\", VOCAB)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AzgzUNfi2W9l"},"source":["Keras를 통한 전처리 과정\n","1. Text를 tokenize하여 id 값으로 변경해 줍니다. (tokenizer.texts_to_sequences)\n","2. id로 변경해준 문장들을 모두 문장 최대 길이로 padding 처리해 줍니다. (pad_sequences)"]},{"cell_type":"code","metadata":{"id":"2q3aiKz-1UBU"},"source":["tokenizer = Tokenizer(num_words=len(VOCAB), lower=True, char_level=False)\n","tokenizer.fit_on_texts(TRAIN_SENTENCES)\n","TRAIN_SEQUENCES = tokenizer.texts_to_sequences(TRAIN_SENTENCES)\n","TEST_SEQUENCES = tokenizer.texts_to_sequences(TEST_SENTENCES)\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","\n","print(TRAIN_SENTENCES[2])\n","print(TRAIN_SEQUENCES[2])\n","\n","X_train = pad_sequences(TRAIN_SEQUENCES, padding='post', maxlen=MAX_SEQUENCE_LEN)\n","X_test = pad_sequences(TEST_SEQUENCES, padding='post', maxlen=MAX_SEQUENCE_LEN)\n","print(\"PAD_SEQUENCES COMPLETES\")\n","print(X_train[0])\n","print(MAX_SEQUENCE_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VEGEmta02bnh"},"source":["모델 설정\n","1. Random으로 초기화된 임베딩이 아닌 pre-trained 된 GLoVE 임베딩으로 학습하고자 합니다.\n","2. 따라서 학습 코퍼스에 있는 단어들 중 GLoVE 임베딩에 있는 단어들을 GLoVE 임베딩으로 초기화 해줍니다.\n","3. 본 실험에서는 GLoVE 임베딩 크기가 50인 것과 100인 것을 통해 실험을 진행해 봅니다."]},{"cell_type":"code","metadata":{"id":"G_6kZOfb2dvb"},"source":["def create_embedding_matrix(filepath, word_index, embedding_dim):\n","    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","    with open(filepath) as f:\n","        for line in f:\n","            word, *vector = line.split()\n","            if word in word_index:\n","                idx = word_index[word] \n","                embedding_matrix[idx] = np.array(\n","                    vector, dtype=np.float32)[:embedding_dim]\n","\n","    return embedding_matrix\n","\n","EMBEDDING_DIM = 50 #100\n","embedding_matrix = create_embedding_matrix(\n","    '/content/glove.6B.50d.txt',\n","    tokenizer.word_index, EMBEDDING_DIM\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bE_hSAlb2fxa"},"source":["#accuracy, loss 시각화\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","def plot_history(history):\n","    acc = history.history['acc']\n","    val_acc = history.history['val_acc']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48Wkbajt2kYN"},"source":["CNN 모델 선언\n","<br> 본 모델은 필터의 Window Size가 2, 3, 4, 5인 필터 각 100개씩을 사용해 모델을 학습합니다. 다양한 크기의 필터를 사용하면 성능이 더 올라갈까요?\n","<br>정답은 \"올라갑니다.\" 입니다. Convolution 필터가 보는 단어의 갯수가 다양하게 되기 때문에 문장의 local 정보와 global 정보 모두를 학습할 수 있게 됩니다. 그럼 실험을 통해 확인해 볼까요?"]},{"cell_type":"code","metadata":{"id":"8A9It5yX2p7U"},"source":["from keras.models import Sequential\n","from keras import layers\n","from keras.models import Model\n","\n","seq_input = layers.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')\n","seq_embedded = layers.Embedding(VOCAB_SIZE, \n","                           EMBEDDING_DIM, \n","                           weights=[embedding_matrix], \n","                           input_length=MAX_SEQUENCE_LEN, \n","                           trainable=True)(seq_input)\n","\n","filters = [2,3,4,5]\n","conv_models = []\n","for filter in filters:\n","  conv_feat = layers.Conv1D(filters=100, \n","                            kernel_size=filter, \n","                            activation='relu',\n","                            padding='valid')(seq_embedded)\n","  pooled_feat = layers.GlobalMaxPooling1D()(conv_feat)\n","  conv_models.append(pooled_feat)\n","\n","conv_merged = layers.concatenate(conv_models, axis=1)\n","\n","model_output = layers.Dropout(0.2)(conv_merged)\n","model_output = layers.Dense(10, activation='relu')(model_output)\n","logits = layers.Dense(1, activation='sigmoid')(model_output)\n","\n","model = Model(seq_input, logits)\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model.summary()\n","\n","#학습 시작\n","history = model.fit(X_train, TRAIN_LABELS,\n","                    epochs=10,\n","                    verbose=True,\n","                    validation_data=(X_test, TEST_LABELS),\n","                    batch_size=128)\n","# 결과 시각화\n","plot_history(history)"],"execution_count":null,"outputs":[]}]}